{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average words per sentence of different subreddits\n",
    "\n",
    "### Background\n",
    "An analysis of the average word count per sentence of comments from the most popular Subreddits.\n",
    "\n",
    "### Subreddits analyzed (top 20 most subscribed as of April 2019):\n",
    "\n",
    "* r/mildlyinteresting\n",
    "* r/funny\n",
    "* r/AskReddit\n",
    "* r/gaming\n",
    "* r/pics\n",
    "* r/science\n",
    "* r/worldnews\n",
    "* r/todayilearned\n",
    "* r/movies\n",
    "* r/aww\n",
    "* r/videos\n",
    "* r/Music\n",
    "* r/IAmA\n",
    "* r/gifs\n",
    "* r/news\n",
    "* r/EarthPorn\n",
    "* r/askscience\n",
    "* r/blog\n",
    "* r/Showerthoughts\n",
    "* r/explainlikeimfive\n",
    "* r/books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "import string\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-04-06 19:06:07.738456]credfile.json .......... is being used as credfile\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Load credfile and display when last updated\n",
    "credfile = 'credfile.json'\n",
    "credfile_prefix = ''\n",
    "\n",
    "# Read credentials to a dictionary\n",
    "with open(credfile) as fh:\n",
    "    creds = json.loads(fh.read())\n",
    "\n",
    "print(f\"[{datetime.datetime.now()}]\" + f\"{credfile} {'.' * 10} is being used as credfile\")\n",
    "\n",
    "reddit = praw.Reddit(client_id=creds['client_id'],\n",
    "                     client_secret=creds['client_secret'],\n",
    "                     user_agent=creds['user_agent']\n",
    "                    )\n",
    "\n",
    "print(reddit.read_only)  # Output: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_list = [#'AskReddit', \n",
    "                  'funny', \n",
    "                  'gaming', \n",
    "                  'pics', \n",
    "                  'science', \n",
    "                  'worldnews', \n",
    "                  'todayilearned', \n",
    "                  'movies', \n",
    "                  'aww', \n",
    "                  'videos', \n",
    "                  'Music', \n",
    "                  'IAmA', \n",
    "                  'gifs', \n",
    "                  'news', \n",
    "                  'EarthPorn', \n",
    "                  'askscience', \n",
    "                  'blog', \n",
    "                  'Showerthoughts', \n",
    "                  'explainlikeimfive', \n",
    "                  'books']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('AskReddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_words(word_list):\n",
    "    \"\"\"\n",
    "    Remove stopwords, one-caracter words, and convert words to lowercase.\n",
    "    Given a list of tokenized words)\n",
    "    \"\"\"\n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in word_list]\n",
    "    \n",
    "    # convert to lower case\n",
    "    stripped_lower = [word.lower() for word in stripped]\n",
    "\n",
    "    # Remove one character words\n",
    "    text_tokenized = [word for word in stripped_lower if len(word) > 2]\n",
    "\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_stopped = [w for w in text_tokenized if not w in stop_words]\n",
    "    \n",
    "    return text_stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test iterate over one submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_len_comment(comment):\n",
    "    \n",
    "    sentences = sent_tokenize(comment)\n",
    "    words = word_tokenize(comment)   \n",
    "\n",
    "    cleaned_list = [ x for x in words if len(x) > 2 ]\n",
    "    \n",
    "    avg_sent_len = round(len(cleaned_list) / len(sentences), 2)\n",
    "    \n",
    "    return avg_sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission = reddit.submission(id='ba4dyn')\n",
    "\n",
    "# iterate over top comments in the submission and\\= create list of sentences\n",
    "submission.comments.replace_more(limit=None)\n",
    "\n",
    "total_submission_len = 0\n",
    "for top_level_comment in submission.comments[1:]: # Skip AutoMod comment\n",
    "    avg_sent_len = avg_sentence_len_comment(top_level_comment.body.replace('“', '').replace('”', ''))         \n",
    "    total_submission_len += avg_sent_len\n",
    "\n",
    "avg_submission_len = total_submission_len / len(submission.comments[1:])\n",
    "\n",
    "print(avg_submission_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:  2019-04-06 23:41:04.605157\n",
      "Average submission length for funny:  5.54\n",
      "                   Time after funny:  2019-04-06 23:41:37.928244\n",
      "Average submission length for gaming:  8.52\n",
      "                   Time after gaming:  2019-04-06 23:42:07.165828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:prawcore:Retrying due to 500 status: POST https://oauth.reddit.com/api/morechildren/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average submission length for pics:  3.92\n",
      "                   Time after pics:  2019-04-06 23:42:52.001499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:prawcore:Retrying due to 500 status: POST https://oauth.reddit.com/api/morechildren/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission has 0 comments, continuing...\n",
      "Submission has 0 comments, continuing...\n",
      "Average submission length for science:  21.75\n",
      "                   Time after science:  2019-04-06 23:43:38.387803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:prawcore:Retrying due to 500 status: POST https://oauth.reddit.com/api/morechildren/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average submission length for worldnews:  11.59\n",
      "                   Time after worldnews:  2019-04-06 23:47:24.093312\n",
      "Average submission length for todayilearned:  11.64\n",
      "                   Time after todayilearned:  2019-04-06 23:48:50.837310\n",
      "Submission has 0 comments, continuing...\n",
      "Average submission length for movies:  8.81\n",
      "                   Time after movies:  2019-04-06 23:49:46.902955\n",
      "Average submission length for aww:  3.7\n",
      "                   Time after aww:  2019-04-06 23:50:21.906987\n",
      "Average submission length for videos:  8.42\n",
      "                   Time after videos:  2019-04-06 23:50:49.589948\n",
      "Submission has 0 comments, continuing...\n",
      "Average submission length for Music:  56.2\n",
      "                   Time after Music:  2019-04-06 23:50:55.047474\n",
      "Submission has 0 comments, continuing...\n",
      "Average submission length for IAmA:  9.43\n",
      "                   Time after IAmA:  2019-04-06 23:52:39.627297\n",
      "Average submission length for gifs:  5.54\n",
      "                   Time after gifs:  2019-04-06 23:53:06.112242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:prawcore:Retrying due to 500 status: POST https://oauth.reddit.com/api/morechildren/\n",
      "WARNING:prawcore:Retrying due to 500 status: POST https://oauth.reddit.com/api/morechildren/\n",
      "WARNING:prawcore:Retrying due to 500 status: POST https://oauth.reddit.com/api/morechildren/\n",
      "WARNING:prawcore:Retrying due to 500 status: POST https://oauth.reddit.com/api/morechildren/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average submission length for news:  8.17\n",
      "                   Time after news:  2019-04-06 23:59:41.170705\n",
      "Average submission length for EarthPorn:  5.68\n",
      "                   Time after EarthPorn:  2019-04-06 23:59:45.043248\n",
      "Submission has 0 comments, continuing...\n",
      "Average submission length for askscience:  23.48\n",
      "                   Time after askscience:  2019-04-06 23:59:52.505319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:prawcore:Retrying due to 500 status: POST https://oauth.reddit.com/api/morechildren/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average submission length for blog:  5.78\n",
      "                   Time after blog:  2019-04-07 00:01:56.701576\n",
      "Average submission length for Showerthoughts:  5.51\n",
      "                   Time after Showerthoughts:  2019-04-07 00:02:10.460114\n",
      "Submission has 0 comments, continuing...\n",
      "Submission has 0 comments, continuing...\n",
      "Average submission length for explainlikeimfive:  22.49\n",
      "                   Time after explainlikeimfive:  2019-04-07 00:02:31.289383\n",
      "Average submission length for books:  14.62\n",
      "                   Time after books:  2019-04-07 00:02:39.908727\n",
      "End:    2019-04-07 00:02:39.910908\n"
     ]
    }
   ],
   "source": [
    "lt = 10 # take top 'lt' number of hottest submissions\n",
    "print(\"Start: \", datetime.datetime.now())\n",
    "\n",
    "avg_sentence_length_df = pd.DataFrame(columns=['subreddit', 'avg_comment_sent_length'])\n",
    "counter=0\n",
    "\n",
    "for sub in subreddit_list:\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    total_subreddit_len = 0\n",
    "    for submission in subreddit.hot(limit=lt):\n",
    "        # iterate over top comments in the submission and\\= create list of sentences\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        total_submission_len = 0\n",
    "        for top_level_comment in submission.comments:\n",
    "            avg_sent_len = avg_sentence_len_comment(top_level_comment.body.replace('“', '').replace('”', ''))         \n",
    "            total_submission_len += avg_sent_len\n",
    "        try:\n",
    "            avg_submission_len = total_submission_len / len(submission.comments[1:])\n",
    "        except ZeroDivisionError:\n",
    "            print(\"Submission has 0 comments, continuing...\")\n",
    "            continue\n",
    "            \n",
    "        # keep running total of totals for entire subreddit\n",
    "        total_subreddit_len += avg_submission_len\n",
    "        \n",
    "    # take total / limit\n",
    "    avg_subreddit_len = total_subreddit_len / lt\n",
    "    print(f\"Average submission length for {sub}: \", round(avg_submission_len, 2))\n",
    "    print(f\"                   Time after {sub}: \", datetime.datetime.now())\n",
    "    \n",
    "    avg_sentence_length_df.loc[counter, :] = [sub, avg_subreddit_len]\n",
    "    counter+=1\n",
    "print(\"End:   \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>avg_comment_sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>funny</td>\n",
       "      <td>6.06328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gaming</td>\n",
       "      <td>7.27696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pics</td>\n",
       "      <td>9.72608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>science</td>\n",
       "      <td>12.1036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>9.49657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>todayilearned</td>\n",
       "      <td>9.55195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>movies</td>\n",
       "      <td>7.80287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aww</td>\n",
       "      <td>5.28305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>videos</td>\n",
       "      <td>7.60352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Music</td>\n",
       "      <td>11.5134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>IAmA</td>\n",
       "      <td>8.55715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gifs</td>\n",
       "      <td>5.57712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>news</td>\n",
       "      <td>9.4486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>EarthPorn</td>\n",
       "      <td>8.10053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>askscience</td>\n",
       "      <td>16.7055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>blog</td>\n",
       "      <td>8.62237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Showerthoughts</td>\n",
       "      <td>7.53291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>explainlikeimfive</td>\n",
       "      <td>15.7499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>books</td>\n",
       "      <td>11.8875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            subreddit avg_comment_sent_length\n",
       "0               funny                 6.06328\n",
       "1              gaming                 7.27696\n",
       "2                pics                 9.72608\n",
       "3             science                 12.1036\n",
       "4           worldnews                 9.49657\n",
       "5       todayilearned                 9.55195\n",
       "6              movies                 7.80287\n",
       "7                 aww                 5.28305\n",
       "8              videos                 7.60352\n",
       "9               Music                 11.5134\n",
       "10               IAmA                 8.55715\n",
       "11               gifs                 5.57712\n",
       "12               news                  9.4486\n",
       "13          EarthPorn                 8.10053\n",
       "14         askscience                 16.7055\n",
       "15               blog                 8.62237\n",
       "16     Showerthoughts                 7.53291\n",
       "17  explainlikeimfive                 15.7499\n",
       "18              books                 11.8875"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_sentence_length_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sentence_length_df.to_csv('avg_sentence_length_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
