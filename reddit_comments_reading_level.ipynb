{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average words per sentence of different subreddits\n",
    "\n",
    "### Background\n",
    "An analysis of the average word count per sentence of comments from the most popular Subreddits.\n",
    "\n",
    "### Subreddits analyzed (top 20 most subscribed as of April 2019):\n",
    "\n",
    "* r/mildlyinteresting\n",
    "* r/funny\n",
    "* r/AskReddit\n",
    "* r/gaming\n",
    "* r/pics\n",
    "* r/science\n",
    "* r/worldnews\n",
    "* r/todayilearned\n",
    "* r/movies\n",
    "* r/aww\n",
    "* r/videos\n",
    "* r/Music\n",
    "* r/IAmA\n",
    "* r/gifs\n",
    "* r/news\n",
    "* r/EarthPorn\n",
    "* r/askscience\n",
    "* r/blog\n",
    "* r/Showerthoughts\n",
    "* r/explainlikeimfive\n",
    "* r/books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "import string\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-04-08 07:15:03.085713]credfile.json .......... is being used as credfile\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Load credfile and display when last updated\n",
    "credfile = 'credfile.json'\n",
    "credfile_prefix = ''\n",
    "\n",
    "# Read credentials to a dictionary\n",
    "with open(credfile) as fh:\n",
    "    creds = json.loads(fh.read())\n",
    "\n",
    "print(f\"[{datetime.datetime.now()}]\" + f\"{credfile} {'.' * 10} is being used as credfile\")\n",
    "\n",
    "reddit = praw.Reddit(client_id=creds['client_id'],\n",
    "                     client_secret=creds['client_secret'],\n",
    "                     user_agent=creds['user_agent']\n",
    "                    )\n",
    "\n",
    "print(reddit.read_only)  # Output: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_list = [#'AskReddit', \n",
    "#                   'funny', \n",
    "#                   'gaming', \n",
    "#                   'pics', \n",
    "#                   'science', \n",
    "#                   'worldnews', \n",
    "#                   'todayilearned', \n",
    "#                   'movies', \n",
    "#                   'aww', \n",
    "#                   'videos', \n",
    "#                   'Music', \n",
    "                  'IAmA', \n",
    "                  'gifs', \n",
    "                  'news', \n",
    "                  'EarthPorn', \n",
    "                  'askscience', \n",
    "                  'blog', \n",
    "                  'Showerthoughts', \n",
    "                  'explainlikeimfive', \n",
    "                  'books']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('AskReddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_words(word_list):\n",
    "    \"\"\"\n",
    "    Remove stopwords, one-caracter words, and convert words to lowercase.\n",
    "    Given a list of tokenized words)\n",
    "    \"\"\"\n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in word_list]\n",
    "    \n",
    "    # convert to lower case\n",
    "    stripped_lower = [word.lower() for word in stripped]\n",
    "\n",
    "    # Remove one character words\n",
    "    text_tokenized = [word for word in stripped_lower if len(word) > 2]\n",
    "\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_stopped = [w for w in text_tokenized if not w in stop_words]\n",
    "    \n",
    "    return text_stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test iterate over one submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_len_comment(comment):\n",
    "    \n",
    "    sentences = sent_tokenize(comment)\n",
    "    words = word_tokenize(comment)   \n",
    "\n",
    "    cleaned_list = [ x for x in words if len(x) > 2 ]\n",
    "    \n",
    "    try:\n",
    "        avg_sent_len = round(len(cleaned_list) / len(sentences), 2)\n",
    "    except ZeroDivisionError:\n",
    "        avg_sent_len = 0\n",
    "        \n",
    "    return avg_sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.960803571428567\n"
     ]
    }
   ],
   "source": [
    "submission = reddit.submission(id='ba4dyn')\n",
    "\n",
    "# iterate over top comments in the submission and\\= create list of sentences\n",
    "submission.comments.replace_more(limit=None)\n",
    "\n",
    "total_submission_len = 0\n",
    "for top_level_comment in submission.comments[1:]: # Skip AutoMod comment\n",
    "    avg_sent_len = avg_sentence_len_comment(top_level_comment.body.replace('“', '').replace('”', ''))         \n",
    "    total_submission_len += avg_sent_len\n",
    "\n",
    "avg_submission_len = total_submission_len / len(submission.comments[1:])\n",
    "\n",
    "print(avg_submission_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:  2019-04-08 18:20:47.528881\n"
     ]
    }
   ],
   "source": [
    "lt = 20 # take top 'lt' number of hottest submissions\n",
    "print(\"Start: \", datetime.datetime.now())\n",
    "\n",
    "avg_sentence_length_df = pd.DataFrame(columns=['subreddit', 'avg_comment_sent_length'])\n",
    "counter=0\n",
    "\n",
    "for sub in subreddit_list:\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    total_subreddit_len = 0\n",
    "    start = time.time()\n",
    "    for submission in subreddit.top(limit=lt):\n",
    "        # iterate over top comments in the submission and\\= create list of sentences\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        total_submission_len = 0\n",
    "        for top_level_comment in submission.comments:\n",
    "            avg_sent_len = avg_sentence_len_comment(top_level_comment.body.replace('“', '').replace('”', ''))         \n",
    "            total_submission_len += avg_sent_len\n",
    "        try:\n",
    "            avg_submission_len = total_submission_len / len(submission.comments[1:])\n",
    "        except ZeroDivisionError:\n",
    "            print(\"Submission has 0 comments, continuing...\")\n",
    "            continue\n",
    "            \n",
    "        # keep running total of totals for entire subreddit\n",
    "        total_subreddit_len += avg_submission_len\n",
    "        \n",
    "    # take total / limit\n",
    "    avg_subreddit_len = total_subreddit_len / lt\n",
    "    end = time.time()\n",
    "    net = end-start\n",
    "    print(f\"Average submission comment length for {sub}: \", round(avg_submission_len, 2))\n",
    "    print(f\"                           Time | {sub}: \", net)\n",
    "    \n",
    "    avg_sentence_length_df.loc[counter, :] = [sub, avg_subreddit_len]\n",
    "    counter+=1\n",
    "print(\"End:   \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sentence_length_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sentence_length_df.to_csv('avg_sentence_length_df_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
