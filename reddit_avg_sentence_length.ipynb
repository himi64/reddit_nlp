{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average words per sentence of different subreddits\n",
    "\n",
    "### Background\n",
    "An analysis of the average word count per sentence of comments from the most popular Subreddits.\n",
    "\n",
    "### Subreddits analyzed (top 20 most subscribed as of April 2019):\n",
    "\n",
    "### Contents\n",
    "1. Setup\n",
    "2. Standardize words\n",
    "3. Calculate average words per submission\n",
    "4. Calculate average words per submission for entire subreddit\n",
    "5. View Data\n",
    "6. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "import string\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-04-11 20:45:58.012862]credfile.json .......... is being used as credfile\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Load credfile and display when last updated\n",
    "credfile = 'credfile.json'\n",
    "credfile_prefix = ''\n",
    "\n",
    "# Read credentials to a dictionary\n",
    "with open(credfile) as fh:\n",
    "    creds = json.loads(fh.read())\n",
    "\n",
    "print(f\"[{datetime.datetime.now()}]\" + f\"{credfile} {'.' * 10} is being used as credfile\")\n",
    "\n",
    "reddit = praw.Reddit(client_id=creds['client_id'],\n",
    "                     client_secret=creds['client_secret'],\n",
    "                     user_agent=creds['user_agent']\n",
    "                    )\n",
    "\n",
    "print(reddit.read_only)  # Output: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_list = [#'AskReddit', \n",
    "#                   'funny', \n",
    "#                   'gaming', \n",
    "#                   'pics', \n",
    "#                   'science', \n",
    "#                   'worldnews', \n",
    "#                   'todayilearned', \n",
    "#                   'movies', \n",
    "#                   'soccer', \n",
    "#                   'videos', \n",
    "#                   'Music', \n",
    "#                   'IAmA', \n",
    "#                   'gifs', \n",
    "#                   'news', \n",
    "#                   'EarthPorn', \n",
    "#                   'askscience', \n",
    "                  'blog', \n",
    "                  'Showerthoughts', \n",
    "                  'explainlikeimfive', \n",
    "                  'books']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_words(word_list):\n",
    "    \"\"\"\n",
    "    Remove stopwords, one-caracter words, and convert words to lowercase.\n",
    "    Given a list of tokenized words)\n",
    "    \"\"\"\n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in word_list]\n",
    "    \n",
    "    # convert to lower case\n",
    "    stripped_lower = [word.lower() for word in stripped]\n",
    "\n",
    "    # Remove one character words\n",
    "    text_tokenized = [word for word in stripped_lower if len(word) > 2]\n",
    "\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_stopped = [w for w in text_tokenized if not w in stop_words]\n",
    "    \n",
    "    return text_stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test iterate over one submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_len_comment(comment):\n",
    "    \n",
    "    sentences = sent_tokenize(comment)\n",
    "    words = word_tokenize(comment)   \n",
    "\n",
    "    cleaned_list = [ x for x in words if len(x) > 2 ]\n",
    "    \n",
    "    try:\n",
    "        avg_sent_len = round(len(cleaned_list) / len(sentences), 2)\n",
    "    except ZeroDivisionError:\n",
    "        avg_sent_len = 0\n",
    "        \n",
    "    return avg_sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.932023809523804\n"
     ]
    }
   ],
   "source": [
    "submission = reddit.submission(id='ba4dyn')\n",
    "\n",
    "# iterate over top comments in the submission and\\= create list of sentences\n",
    "submission.comments.replace_more(limit=None)\n",
    "\n",
    "total_submission_len = 0\n",
    "for top_level_comment in submission.comments[1:]: # Skip AutoMod comment\n",
    "    avg_sent_len = avg_sentence_len_comment(top_level_comment.body.replace('“', '').replace('”', ''))         \n",
    "    total_submission_len += avg_sent_len\n",
    "\n",
    "avg_submission_len = total_submission_len / len(submission.comments[1:])\n",
    "\n",
    "print(avg_submission_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.upvote_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_wps_submission(comment_list):\n",
    "    \"\"\"\n",
    "    Iterate over list of comments in a submission and calculate the average words\n",
    "    per sentence of all top-level comments\n",
    "    \"\"\"\n",
    "    total_submission_sentence_len = 0\n",
    "    for comment in comment_list:\n",
    "        avg_sent_len = avg_sentence_len_comment(comment.body.replace('“', '').replace('”', ''))                 \n",
    "        total_submission_sentence_len += avg_sent_len\n",
    "\n",
    "    try:\n",
    "        avg_wps_submission = total_submission_sentence_len / len(comment_list)\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Submission has 0 comments, continuing...\")\n",
    "        avg_wps_submission = 0\n",
    "        \n",
    "    return avg_wps_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_wps_subreddit(submission_list):\n",
    "    \"\"\"\n",
    "    Iterate over all x number of submissions and get the average words \n",
    "    per sentence for the entire subreddit\n",
    "    \"\"\"\n",
    "    total_wps_subreddit = 0\n",
    "    for submission in submission_list:\n",
    "        # iterate over top comments in the submission and create list of sentences\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        avg_wps_per_submission = avg_wps_submission(list(submission.comments))\n",
    "\n",
    "        # keep running total of totals for entire subreddit\n",
    "        total_wps_subreddit += avg_wps_per_submission\n",
    "    \n",
    "    avg_wps_subreddit = total_wps_subreddit / len(submission_list)\n",
    "    \n",
    "    return avg_wps_subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ba4dyn'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submission_data(submission_list):\n",
    "    nested_dict = lambda: defaultdict(nested_dict)\n",
    "    submission_data_dict = nested_dict()\n",
    "    for submission in submission_list:\n",
    "        # iterate over top comments in the submission and create list of sentences\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        submission_data_dict['id']['upvotes'] = submission.ups\n",
    "        submission_data_dict['id']['upvotes_ratio'] = submission.upvote_ratio\n",
    "        submission_data_dict['id']['date'] = datetime.datetime.fromtimestamp(int(submission.created_utc)).strftime(\"%m/%d/%y %H:%M:%S\")\n",
    "        submission_data_dict['id']['comments'] = len(list(submission.comments))\n",
    "        \n",
    "    return submission_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:  2019-04-11 22:25:46.937967\n",
      "Runtime: {round(net, 2)s}\n"
     ]
    }
   ],
   "source": [
    "lt = 1 # take top 'lt' number of hottest submissions\n",
    "print(\"Start: \", datetime.datetime.now())\n",
    "\n",
    "nested_dict = lambda: defaultdict(nested_dict)\n",
    "submission_metadata = nested_dict()\n",
    "avg_sentence_length_df = pd.DataFrame(columns=['subreddit', 'avg_comment_sent_length'])\n",
    "counter = 0\n",
    "start = time.time()\n",
    "for sub in subreddit_list:\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    submission_list = list(subreddit.hot(limit=lt))\n",
    "    avg_wps_per_subreddit = avg_wps_subreddit(submission_list)\n",
    "    # take total / limit\n",
    "    avg_sentence_length_df.loc[counter, :] = [sub, avg_wps_per_subreddit]\n",
    "    # record metadata\n",
    "    submission_metadata[sub] = get_submission_data(submission_list)\n",
    "    counter+=1\n",
    "    \n",
    "end = time.time()\n",
    "net = net = end-start\n",
    "\n",
    "print(\"Runtime: {round(net, 2)s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>avg_comment_sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blog</td>\n",
       "      <td>9.54075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Showerthoughts</td>\n",
       "      <td>8.59443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>explainlikeimfive</td>\n",
       "      <td>8.87231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>books</td>\n",
       "      <td>10.242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           subreddit avg_comment_sent_length\n",
       "0               blog                 9.54075\n",
       "1     Showerthoughts                 8.59443\n",
       "2  explainlikeimfive                 8.87231\n",
       "3              books                  10.242"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_sentence_length_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blog': defaultdict(<function __main__.get_submission_data.<locals>.<lambda>()>,\n",
       "             {'id': defaultdict(<function __main__.get_submission_data.<locals>.<lambda>()>,\n",
       "                          {'upvotes': 37261,\n",
       "                           'upvotes_ratio': 0.84,\n",
       "                           'date': '04/08/19 10:34:10',\n",
       "                           'comments': 307})}),\n",
       " 'Showerthoughts': defaultdict(<function __main__.get_submission_data.<locals>.<lambda>()>,\n",
       "             {'id': defaultdict(<function __main__.get_submission_data.<locals>.<lambda>()>,\n",
       "                          {'upvotes': 4909,\n",
       "                           'upvotes_ratio': 0.99,\n",
       "                           'date': '02/13/18 17:46:38',\n",
       "                           'comments': 131})}),\n",
       " 'explainlikeimfive': defaultdict(<function __main__.get_submission_data.<locals>.<lambda>()>,\n",
       "             {'id': defaultdict(<function __main__.get_submission_data.<locals>.<lambda>()>,\n",
       "                          {'upvotes': 3944,\n",
       "                           'upvotes_ratio': 0.95,\n",
       "                           'date': '04/11/19 10:20:38',\n",
       "                           'comments': 15})}),\n",
       " 'books': defaultdict(<function __main__.get_submission_data.<locals>.<lambda>()>,\n",
       "             {'id': defaultdict(<function __main__.get_submission_data.<locals>.<lambda>()>,\n",
       "                          {'upvotes': 95,\n",
       "                           'upvotes_ratio': 0.98,\n",
       "                           'date': '03/30/19 09:10:33',\n",
       "                           'comments': 10})})}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(submission_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sentence_length_df.to_csv('avg_wps_hottest_20_n1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
