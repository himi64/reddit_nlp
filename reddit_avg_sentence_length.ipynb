{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average words per sentence of different subreddits\n",
    "\n",
    "### Background\n",
    "An analysis of the average word count per sentence of comments from the most popular Subreddits. \n",
    "\n",
    "wps = words per sentence\n",
    "\n",
    "### Subreddits analyzed (top 20 most subscribed as of April 2019):\n",
    "\n",
    "### Contents\n",
    "1. [Setup](#1.-Setup)\n",
    "2. [Average Words Per Sentence for Comments](#2-Average-Words-Per-Sentence-for-Comments)\n",
    "3. [Average wps for Submission](#3.-Average-wps-for-submission)\n",
    "4. [Average wps for Subreddit](#4.-Average-wps-for-subreddit)\n",
    "5. [Data Visualization](#5.-Data-Visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "import string\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-04-13 14:32:17.493945]credfile.json .......... is being used as credfile\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Load credfile and display when last updated\n",
    "credfile = 'credfile.json'\n",
    "credfile_prefix = ''\n",
    "\n",
    "# Read credentials to a dictionary\n",
    "with open(credfile) as fh:\n",
    "    creds = json.loads(fh.read())\n",
    "\n",
    "print(f\"[{datetime.datetime.now()}]\" + f\"{credfile} {'.' * 10} is being used as credfile\")\n",
    "\n",
    "reddit = praw.Reddit(client_id=creds['client_id'],\n",
    "                     client_secret=creds['client_secret'],\n",
    "                     user_agent=creds['user_agent']\n",
    "                    )\n",
    "\n",
    "print(reddit.read_only)  # Output: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_list = [#'AskReddit', \n",
    "#                   'funny', \n",
    "#                   'gaming', \n",
    "#                   'pics', \n",
    "#                   'science', \n",
    "#                   'worldnews', \n",
    "#                   'todayilearned', \n",
    "#                   'movies', \n",
    "#                   'soccer', \n",
    "#                   'videos', \n",
    "#                   'Music', \n",
    "#                   'IAmA', \n",
    "#                   'gifs', \n",
    "#                   'news', \n",
    "#                   'EarthPorn', \n",
    "#                   'askscience', \n",
    "                  'blog', \n",
    "                  'Showerthoughts', \n",
    "                  'explainlikeimfive', \n",
    "                  'books']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Average Words Per Sentence for Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_words(word_list):\n",
    "    \"\"\"\n",
    "    Remove stopwords, one-caracter words, and convert words to lowercase.\n",
    "    Given a list of tokenized words)\n",
    "    \"\"\"\n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in word_list]\n",
    "    \n",
    "    # convert to lower case\n",
    "    stripped_lower = [word.lower() for word in stripped]\n",
    "\n",
    "    # Remove one character words\n",
    "    text_tokenized = [word for word in stripped_lower if len(word) > 2]\n",
    "\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_stopped = [w for w in text_tokenized if not w in stop_words]\n",
    "    \n",
    "    return text_stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_len_comment(comment):\n",
    "    \n",
    "    sentences = sent_tokenize(comment)\n",
    "    words = word_tokenize(comment)   \n",
    "\n",
    "    cleaned_list = [ x for x in words if len(x) > 2 ]\n",
    "    \n",
    "    try:\n",
    "        avg_sent_len = round(len(cleaned_list) / len(sentences), 2)\n",
    "    except ZeroDivisionError:\n",
    "        avg_sent_len = 0\n",
    "        \n",
    "    return avg_sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission comments average wps:  11.932023809523804\n",
      "Submission upvote ratio:  0.88\n"
     ]
    }
   ],
   "source": [
    "submission = reddit.submission(id='ba4dyn')\n",
    "\n",
    "# iterate over top comments in the submission and\\= create list of sentences\n",
    "submission.comments.replace_more(limit=None)\n",
    "\n",
    "total_submission_len = 0\n",
    "for top_level_comment in submission.comments[1:]: # Skip AutoMod comment\n",
    "    avg_sent_len = avg_sentence_len_comment(top_level_comment.body.replace('“', '').replace('”', ''))         \n",
    "    total_submission_len += avg_sent_len\n",
    "\n",
    "avg_submission_len = total_submission_len / len(submission.comments[1:])\n",
    "\n",
    "print(\"Submission comments average wps: \", avg_submission_len)\n",
    "print(\"Submission upvote ratio: \", submission.upvote_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Average wps for Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on one submission using `id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_wps_submission(comment_list):\n",
    "    \"\"\"\n",
    "    Iterate over list of comments in a submission and calculate the average words\n",
    "    per sentence of all top-level comments\n",
    "    \"\"\"\n",
    "    total_submission_sentence_len = 0\n",
    "    for comment in comment_list:\n",
    "        avg_sent_len = avg_sentence_len_comment(comment.body.replace('“', '').replace('”', ''))                 \n",
    "        total_submission_sentence_len += avg_sent_len\n",
    "\n",
    "    try:\n",
    "        avg_wps_submission = total_submission_sentence_len / len(comment_list)\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Submission has 0 comments, continuing...\")\n",
    "        avg_wps_submission = 0\n",
    "        \n",
    "    return avg_wps_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Average wps for Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_wps_subreddit(submission_list):\n",
    "    \"\"\"\n",
    "    Iterate over all x number of submissions and get the average words \n",
    "    per sentence for the entire subreddit\n",
    "    \"\"\"\n",
    "    total_wps_subreddit = 0\n",
    "    for submission in submission_list:\n",
    "        # iterate over top comments in the submission and create list of sentences\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        avg_wps_per_submission = avg_wps_submission(list(submission.comments))\n",
    "\n",
    "        # keep running total of totals for entire subreddit\n",
    "        total_wps_subreddit += avg_wps_per_submission\n",
    "    \n",
    "    avg_wps_subreddit = total_wps_subreddit / len(submission_list)\n",
    "    \n",
    "    return avg_wps_subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submission_data(submission_list):\n",
    "    nested_dict = lambda: defaultdict(nested_dict)\n",
    "    submission_data_dict = nested_dict()\n",
    "    for submission in submission_list:\n",
    "        # iterate over top comments in the submission and create list of sentences\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        submission_data_dict['id']['upvotes'] = submission.ups\n",
    "        submission_data_dict['id']['upvotes_ratio'] = submission.upvote_ratio\n",
    "        submission_data_dict['id']['date'] = datetime.datetime.fromtimestamp(int(submission.created_utc)).strftime(\"%m/%d/%y %H:%M:%S\")\n",
    "        submission_data_dict['id']['comments'] = len(list(submission.comments))\n",
    "        submission_data_dict['id']['avg_comment_wps'] = round(avg_wps_submission(list(submission.comments)), 2)\n",
    "        \n",
    "    return submission_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate over the list of Subreddits\n",
    "`lt` is the number of submissions returned per subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_over_subs(subreddit_list, method, limit):\n",
    "    \n",
    "    lt = limit\n",
    "    nested_dict = lambda: defaultdict(nested_dict)\n",
    "    submission_metadata = nested_dict()\n",
    "    start = time.time()\n",
    "    for sub in subreddit_list:\n",
    "        subreddit = reddit.subreddit(sub)\n",
    "        if method == \"hot\":\n",
    "            submission_list = list(subreddit.hot(limit=limit))\n",
    "        elif method == \"controversial\":\n",
    "            submission_list = list(subreddit.controversial(limit=limit))\n",
    "        elif method == \"top\":\n",
    "            submission_list = list(subreddit.top(limit=limit))\n",
    "        else:\n",
    "            raise ValueError\n",
    "            print(\"Please select a valid type from: top, hot, controversial\")\n",
    "        submission_metadata[sub] = get_submission_data(submission_list)\n",
    "\n",
    "    end = time.time()\n",
    "    net = net = end-start\n",
    "\n",
    "    print(f\"Runtime: {round(net, 2)}s\")\n",
    "    \n",
    "    return submission_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = iterate_over_subs(subreddit_list, \"hot\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 55.31s\n"
     ]
    }
   ],
   "source": [
    "lt = 1\n",
    "\n",
    "nested_dict = lambda: defaultdict(nested_dict)\n",
    "submission_metadata = nested_dict()\n",
    "start = time.time()\n",
    "for sub in subreddit_list:\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    submission_list = list(subreddit.hot(limit=lt))\n",
    "    submission_metadata[sub] = get_submission_data(submission_list)\n",
    "    \n",
    "end = time.time()\n",
    "net = net = end-start\n",
    "\n",
    "print(f\"Runtime: {round(net, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_to_regular(d):\n",
    "    if isinstance(d, defaultdict):\n",
    "        d = {k: default_to_regular(v) for k, v in d.items()}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blog': {'id': {'upvotes': 37278,\n",
       "   'upvotes_ratio': 0.84,\n",
       "   'date': '04/08/19 10:34:10',\n",
       "   'comments': 306,\n",
       "   'avg_comment_wps': 9.536797385620917}},\n",
       " 'Showerthoughts': {'id': {'upvotes': 4910,\n",
       "   'upvotes_ratio': 0.99,\n",
       "   'date': '02/13/18 17:46:38',\n",
       "   'comments': 131,\n",
       "   'avg_comment_wps': 8.59442748091603}},\n",
       " 'explainlikeimfive': {'id': {'upvotes': 6211,\n",
       "   'upvotes_ratio': 0.94,\n",
       "   'date': '04/13/19 02:25:24',\n",
       "   'comments': 14,\n",
       "   'avg_comment_wps': 16.917142857142856}},\n",
       " 'books': {'id': {'upvotes': 108,\n",
       "   'upvotes_ratio': 0.99,\n",
       "   'date': '03/30/19 09:10:33',\n",
       "   'comments': 11,\n",
       "   'avg_comment_wps': 10.083636363636364}}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_metadata_dict = default_to_regular(submission_metadata)\n",
    "submission_metadata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': {'upvotes': 108,\n",
       "  'upvotes_ratio': 0.99,\n",
       "  'date': '03/30/19 09:10:33',\n",
       "  'comments': 11,\n",
       "  'avg_comment_wps': 10.083636363636364}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_metadata_dict['books']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sentence_length_df.to_csv('avg_wps_hottest_20_n1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
