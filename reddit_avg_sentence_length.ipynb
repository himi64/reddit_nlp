{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average words per sentence of different subreddits\n",
    "\n",
    "### Background\n",
    "An analysis of the average word count per sentence of comments from the most popular Subreddits.\n",
    "\n",
    "### Subreddits analyzed (top 20 most subscribed as of April 2019):\n",
    "\n",
    "* r/mildlyinteresting\n",
    "* r/funny\n",
    "* r/AskReddit\n",
    "* r/gaming\n",
    "* r/pics\n",
    "* r/science\n",
    "* r/worldnews\n",
    "* r/todayilearned\n",
    "* r/movies\n",
    "* r/soccer\n",
    "* r/videos\n",
    "* r/Music\n",
    "* r/IAmA\n",
    "* r/gifs\n",
    "* r/news\n",
    "* r/EarthPorn\n",
    "* r/askscience\n",
    "* r/blog\n",
    "* r/Showerthoughts\n",
    "* r/explainlikeimfive\n",
    "* r/books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "import string\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-04-08 22:38:37.720977]credfile.json .......... is being used as credfile\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Load credfile and display when last updated\n",
    "credfile = 'credfile.json'\n",
    "credfile_prefix = ''\n",
    "\n",
    "# Read credentials to a dictionary\n",
    "with open(credfile) as fh:\n",
    "    creds = json.loads(fh.read())\n",
    "\n",
    "print(f\"[{datetime.datetime.now()}]\" + f\"{credfile} {'.' * 10} is being used as credfile\")\n",
    "\n",
    "reddit = praw.Reddit(client_id=creds['client_id'],\n",
    "                     client_secret=creds['client_secret'],\n",
    "                     user_agent=creds['user_agent']\n",
    "                    )\n",
    "\n",
    "print(reddit.read_only)  # Output: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_list = ['AskReddit', \n",
    "                  'funny', \n",
    "                  'gaming', \n",
    "                  'pics', \n",
    "                  'science', \n",
    "                  'worldnews', \n",
    "                  'todayilearned', \n",
    "                  'movies', \n",
    "                  'soccer', \n",
    "                  'videos', \n",
    "                  'Music', \n",
    "                  'IAmA', \n",
    "                  'gifs', \n",
    "                  'news', \n",
    "                  'EarthPorn', \n",
    "                  'askscience', \n",
    "                  'blog', \n",
    "                  'Showerthoughts', \n",
    "                  'explainlikeimfive', \n",
    "                  'books']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('AskReddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_words(word_list):\n",
    "    \"\"\"\n",
    "    Remove stopwords, one-caracter words, and convert words to lowercase.\n",
    "    Given a list of tokenized words)\n",
    "    \"\"\"\n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in word_list]\n",
    "    \n",
    "    # convert to lower case\n",
    "    stripped_lower = [word.lower() for word in stripped]\n",
    "\n",
    "    # Remove one character words\n",
    "    text_tokenized = [word for word in stripped_lower if len(word) > 2]\n",
    "\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_stopped = [w for w in text_tokenized if not w in stop_words]\n",
    "    \n",
    "    return text_stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test iterate over one submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_len_comment(comment):\n",
    "    \n",
    "    sentences = sent_tokenize(comment)\n",
    "    words = word_tokenize(comment)   \n",
    "\n",
    "    cleaned_list = [ x for x in words if len(x) > 2 ]\n",
    "    \n",
    "    try:\n",
    "        avg_sent_len = round(len(cleaned_list) / len(sentences), 2)\n",
    "    except ZeroDivisionError:\n",
    "        avg_sent_len = 0\n",
    "        \n",
    "    return avg_sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.939806547619044\n"
     ]
    }
   ],
   "source": [
    "submission = reddit.submission(id='ba4dyn')\n",
    "\n",
    "# iterate over top comments in the submission and\\= create list of sentences\n",
    "submission.comments.replace_more(limit=None)\n",
    "\n",
    "total_submission_len = 0\n",
    "for top_level_comment in submission.comments[1:]: # Skip AutoMod comment\n",
    "    avg_sent_len = avg_sentence_len_comment(top_level_comment.body.replace('“', '').replace('”', ''))         \n",
    "    total_submission_len += avg_sent_len\n",
    "\n",
    "avg_submission_len = total_submission_len / len(submission.comments[1:])\n",
    "\n",
    "print(avg_submission_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.upvote_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_wps_submission(comment_list):\n",
    "    \"\"\"\n",
    "    Iterate over list of comments in a submission and calculate the average words\n",
    "    per sentence of all top-level comments\n",
    "    \"\"\"\n",
    "    total_submission_sentence_len = 0\n",
    "    for comment in comment_list:\n",
    "        avg_sent_len = avg_sentence_len_comment(comment.body.replace('“', '').replace('”', ''))                 \n",
    "        total_submission_sentence_len += avg_sent_len\n",
    "\n",
    "    try:\n",
    "        avg_wps_submission = total_submission_sentence_len / len(comment_list)\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Submission has 0 comments, continuing...\")\n",
    "        avg_wps_submission = 0\n",
    "        \n",
    "    return avg_wps_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_wps_subreddit(submission_list):\n",
    "    \"\"\"\n",
    "    Iterate over all x number of submissions and get the average words \n",
    "    per sentence for the entire subreddit\n",
    "    \"\"\"\n",
    "    total_wps_subreddit = 0\n",
    "    for submission in submission_list:\n",
    "        # iterate over top comments in the submission and create list of sentences\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        avg_wps_per_submission = avg_wps_submission(list(submission.comments))\n",
    "\n",
    "        # keep running total of totals for entire subreddit\n",
    "        total_wps_subreddit += avg_wps_per_submission\n",
    "    \n",
    "    avg_wps_subreddit = total_wps_subreddit / len(submission_list)\n",
    "    \n",
    "    return avg_wps_subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:  2019-04-08 22:47:35.038030\n",
      "AskReddit | Comment Length Avg:  10.21\n",
      "AskReddit | Time              : 112.15s\n",
      "funny | Comment Length Avg:  7.62\n",
      "funny | Time              : 2.16s\n",
      "gaming | Comment Length Avg:  10.29\n",
      "gaming | Time              : 0.41s\n",
      "pics | Comment Length Avg:  18.8\n",
      "pics | Time              : 0.29s\n",
      "science | Comment Length Avg:  9.6\n",
      "science | Time              : 1.87s\n",
      "worldnews | Comment Length Avg:  7.25\n",
      "worldnews | Time              : 52.38s\n",
      "todayilearned | Comment Length Avg:  8.92\n",
      "todayilearned | Time              : 22.43s\n",
      "movies | Comment Length Avg:  1.0\n",
      "movies | Time              : 0.21s\n",
      "Submission has 0 comments, continuing...\n",
      "aww | Comment Length Avg:  0.0\n",
      "aww | Time              : 0.15s\n",
      "videos | Comment Length Avg:  8.03\n",
      "videos | Time              : 44.27s\n",
      "Music | Comment Length Avg:  8.67\n",
      "Music | Time              : 0.43s\n",
      "IAmA | Comment Length Avg:  7.8\n",
      "IAmA | Time              : 0.54s\n",
      "gifs | Comment Length Avg:  4.83\n",
      "gifs | Time              : 0.25s\n",
      "news | Comment Length Avg:  9.18\n",
      "news | Time              : 21.59s\n",
      "EarthPorn | Comment Length Avg:  6.71\n",
      "EarthPorn | Time              : 0.65s\n",
      "askscience | Comment Length Avg:  25.0\n",
      "askscience | Time              : 0.36s\n",
      "blog | Comment Length Avg:  9.81\n",
      "blog | Time              : 31.93s\n",
      "Showerthoughts | Comment Length Avg:  8.59\n",
      "Showerthoughts | Time              : 1.48s\n",
      "explainlikeimfive | Comment Length Avg:  15.74\n",
      "explainlikeimfive | Time              : 0.51s\n",
      "books | Comment Length Avg:  10.18\n",
      "books | Time              : 0.37s\n",
      "End:    2019-04-08 22:52:29.512744\n"
     ]
    }
   ],
   "source": [
    "lt = 1 # take top 'lt' number of hottest submissions\n",
    "print(\"Start: \", datetime.datetime.now())\n",
    "\n",
    "avg_sentence_length_df = pd.DataFrame(columns=['subreddit', 'avg_comment_sent_length'])\n",
    "counter=0\n",
    "\n",
    "for sub in subreddit_list:\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    start = time.time()\n",
    "    avg_wps_per_subreddit = avg_wps_subreddit(list(subreddit.hot(limit=lt)))\n",
    "    # take total / limit\n",
    "    end = time.time()\n",
    "    net = end-start\n",
    "    print(f\"{sub} | Comment Length Avg: \", round(avg_wps_per_subreddit, 2))\n",
    "    print(f\"{sub} | Time              : {round(net, 2)}s\")\n",
    "    \n",
    "    avg_sentence_length_df.loc[counter, :] = [sub, avg_wps_per_subreddit]\n",
    "    counter+=1\n",
    "print(\"End:   \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>avg_comment_sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AskReddit</td>\n",
       "      <td>10.2082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>funny</td>\n",
       "      <td>7.61798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gaming</td>\n",
       "      <td>10.2892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pics</td>\n",
       "      <td>18.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>9.59855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>worldnews</td>\n",
       "      <td>7.24705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>todayilearned</td>\n",
       "      <td>8.91853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>movies</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aww</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>videos</td>\n",
       "      <td>8.02785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Music</td>\n",
       "      <td>8.67304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>IAmA</td>\n",
       "      <td>7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gifs</td>\n",
       "      <td>4.83333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>news</td>\n",
       "      <td>9.18353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>EarthPorn</td>\n",
       "      <td>6.70735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>askscience</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>blog</td>\n",
       "      <td>9.81331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Showerthoughts</td>\n",
       "      <td>8.59443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>explainlikeimfive</td>\n",
       "      <td>15.741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>books</td>\n",
       "      <td>10.175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            subreddit avg_comment_sent_length\n",
       "0           AskReddit                 10.2082\n",
       "1               funny                 7.61798\n",
       "2              gaming                 10.2892\n",
       "3                pics                    18.8\n",
       "4             science                 9.59855\n",
       "5           worldnews                 7.24705\n",
       "6       todayilearned                 8.91853\n",
       "7              movies                       1\n",
       "8                 aww                       0\n",
       "9              videos                 8.02785\n",
       "10              Music                 8.67304\n",
       "11               IAmA                     7.8\n",
       "12               gifs                 4.83333\n",
       "13               news                 9.18353\n",
       "14          EarthPorn                 6.70735\n",
       "15         askscience                      25\n",
       "16               blog                 9.81331\n",
       "17     Showerthoughts                 8.59443\n",
       "18  explainlikeimfive                  15.741\n",
       "19              books                  10.175"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_sentence_length_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sentence_length_df.to_csv('avg_sentence_length_hottest_.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
